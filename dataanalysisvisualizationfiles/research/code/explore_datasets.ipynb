{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset can be found here: https://www.kaggle.com/datasets/vishakhdapat/imdb-movie-reviews?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "imdb_data = pd.read_csv('../data/IMDB Dataset.csv')\n",
    "\n",
    "# Sentiment Distribution\n",
    "def sentiment_distribution(data):\n",
    "    sentiment_counts = data['sentiment'].value_counts()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# Text Analysis\n",
    "def text_analysis(data):\n",
    "    # Review Length Distribution\n",
    "    data['review_length'] = data['review'].apply(len)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data['review_length'], kde=True, color='blue')\n",
    "    plt.title('Review Length Distribution')\n",
    "    plt.xlabel('Review Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Most Common Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = ' '.join(data['review']).lower().split()\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common(20)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=[word[1] for word in common_words], y=[word[0] for word in common_words], palette='inferno')\n",
    "    plt.title('Most Common Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "    # Word Cloud for Positive and Negative Reviews\n",
    "    positive_reviews = data[data['sentiment'] == 'positive']['review'].str.cat(sep=' ')\n",
    "    negative_reviews = data[data['sentiment'] == 'negative']['review'].str.cat(sep=' ')\n",
    "\n",
    "    wordcloud_positive = WordCloud(stopwords=stop_words, background_color='white', colormap='Blues').generate(positive_reviews)\n",
    "    wordcloud_negative = WordCloud(stopwords=stop_words, background_color='white', colormap='Reds').generate(negative_reviews)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "    plt.title('Word Cloud for Positive Reviews')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
    "    plt.title('Word Cloud for Negative Reviews')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Additional Insights\n",
    "def additional_insights(data):\n",
    "    # Average review length by sentiment\n",
    "    avg_length_by_sentiment = data.groupby('sentiment')['review_length'].mean()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=avg_length_by_sentiment.index, y=avg_length_by_sentiment.values, palette='coolwarm')\n",
    "    plt.title('Average Review Length by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Average Review Length')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_distribution(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analysis(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_insights(imdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the CSV file\n",
    "imdb_data = pd.read_csv('../data/IMDB Dataset.csv')\n",
    "\n",
    "# Encode the target labels\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Split data into a pool of unlabeled data and a small initial training set\n",
    "X_pool, X_initial, y_pool, y_initial = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.05, random_state=42)\n",
    "\n",
    "# Vectorize the initial training set\n",
    "X_initial_vectorized = vectorizer.fit_transform(X_initial).toarray()\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the initial training set\n",
    "model.fit(X_initial_vectorized, y_initial)\n",
    "\n",
    "# Define the number of iterations and the sample size for each iteration\n",
    "iterations = 10\n",
    "sample_size = 100\n",
    "\n",
    "# Active Learning Loop\n",
    "for i in range(iterations):\n",
    "    # Randomly sample a subset from the pool\n",
    "    sample_indices = random.sample(range(len(X_pool)), sample_size)\n",
    "    X_sampled = X_pool.iloc[sample_indices]\n",
    "    y_sampled = y_pool.iloc[sample_indices]\n",
    "\n",
    "    # Vectorize the sampled data\n",
    "    X_sampled_vectorized = vectorizer.transform(X_sampled).toarray()\n",
    "\n",
    "    # Update the training data\n",
    "    X_initial_vectorized = np.vstack((X_initial_vectorized, X_sampled_vectorized))\n",
    "    y_initial = np.hstack((y_initial, y_sampled))\n",
    "\n",
    "    # Shuffle the updated training data\n",
    "    X_initial_vectorized, y_initial = shuffle(X_initial_vectorized, y_initial, random_state=42)\n",
    "\n",
    "    # Retrain the model on the updated training set\n",
    "    model.fit(X_initial_vectorized, y_initial)\n",
    "\n",
    "    # Evaluate the model on a test set\n",
    "    X_test_vectorized = vectorizer.transform(imdb_data['review']).toarray()\n",
    "    y_test = imdb_data['sentiment']\n",
    "    y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "    # Output the current iteration's performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {i+1}/{iterations} - Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Remove the sampled instances from the pool\n",
    "    X_pool = X_pool.drop(X_sampled.index)\n",
    "    y_pool = y_pool.drop(y_sampled.index)\n",
    "\n",
    "    # Check if the pool is empty\n",
    "    if len(X_pool) == 0:\n",
    "        print(\"No more data to sample from.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Cancer ISIC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset can be found here: https://www.kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
