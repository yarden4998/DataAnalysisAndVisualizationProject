{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-09-21 15:26:14.730160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-21 15:26:14.730205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-21 15:26:14.731469: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-21 15:26:14.739067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-21 15:26:15.799276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from preprocess_data import preprocess_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model_name):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for text classification using a pre-trained sentence-transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    model_name (str): The name of the pre-trained sentence-transformer model to use for encoding text descriptions.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train_full (numpy array): Original training data embeddings.\n",
    "        - X_test (numpy array): Original test data embeddings.\n",
    "        - X_train_full_normalized (numpy array): Normalized training data embeddings.\n",
    "        - X_test_normalized (numpy array): Normalized test data embeddings.\n",
    "        - y_train_full (numpy array): Training labels.\n",
    "        - y_test (numpy array): Test labels.\n",
    "    \"\"\"\n",
    "    data = preprocess_data()\n",
    "\n",
    "    data['num_genres'] = data['genre'].apply(len)\n",
    "    data = data[data['num_genres'] == 1]\n",
    "\n",
    "    descriptions = data['description_processed'].tolist()\n",
    "    genres = data['genre'].tolist()\n",
    "\n",
    "    descriptions = data['description_processed'].tolist()\n",
    "    genres = data['genre'].tolist()\n",
    "    # Encode the genres as numerical labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(genres)\n",
    "\n",
    "    # Load a pre-trained sentence-transformer model to convert text to embeddings\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Convert descriptions to vector embeddings\n",
    "    X = model.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalize the embeddings for cosine similarity\n",
    "    X_train_full_normalized = normalize(X_train_full, axis=1, norm='l2')\n",
    "    X_test_normalized = normalize(X_test, axis=1, norm='l2')\n",
    "\n",
    "    return X_train_full,X_test, X_train_full_normalized,X_test_normalized,y_train_full,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(iterations,clf,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Perform active learning with random sampling to iteratively improve a classifier's performance.\n",
    "\n",
    "    Parameters:\n",
    "    iterations (int): The number of active learning iterations to perform.\n",
    "    clf (classifier object): The classifier to be trained. \n",
    "    X_test (numpy array): The test data on which to evaluate the classifier's performance.\n",
    "    y_test (numpy array): The true labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    None: The function prints accuracy after each iteration and a final classification report.\n",
    "    \"\"\"\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Incrementally train the classifier with the current training set\n",
    "        clf.partial_fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate performance on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}: Accuracy = {accuracy_scores[-1]:.4f}\")\n",
    "\n",
    "        # Random sampling of new data points\n",
    "        sample_size = min(sample_size, len(X_pool))\n",
    "        selected_indices = np.random.choice(len(X_pool), size=sample_size, replace=False)\n",
    "        X_sample = X_pool[selected_indices]\n",
    "\n",
    "        # Handle potential differences in data type or format\n",
    "        try:\n",
    "            y_sample = y_pool[selected_indices]\n",
    "        except:\n",
    "            y_sample = y_pool.iloc[selected_indices]\n",
    "        \n",
    "        # Update the training set with new samples\n",
    "        X_train = np.vstack([X_train, X_sample])\n",
    "        y_train = np.concatenate([y_train, y_sample])\n",
    "\n",
    "        # Remove the selected samples from the pool\n",
    "        X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "        y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "        # Stop criteria based on the pool size or maximum training size\n",
    "        if len(X_pool) == 0 or len(X_pool) < sample_size or len(X_train) >= 60000:\n",
    "            break\n",
    "\n",
    "    # Print the final classification report\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_as_clustering(iterations,dynamic_clusters,embedding_dim,X_train_full_normalized,clf,budget_per_iteration,uncertainty_threshold,y_train_full,index,X_test_normalized,y_test):\n",
    "    \"\"\"\n",
    "    Perform active learning using Approximate Nearest Neighbors (ANN) as a clustering strategy.\n",
    "\n",
    "    Parameters:\n",
    "    iterations (int): The number of active learning iterations to perform.\n",
    "    dynamic_clusters (int): The initial number of clusters, which increases with each iteration.\n",
    "    embedding_dim (int): The dimensionality of the embedding space.\n",
    "    X_train_full_normalized (numpy array): The normalized full training data embeddings.\n",
    "    clf (classifier object): The classifier to be trained incrementally.\n",
    "    budget_per_iteration (int): The maximum number of samples to be added to the training set per iteration.\n",
    "    uncertainty_threshold (float): The threshold for selecting uncertain samples within each cluster.\n",
    "    y_train_full (numpy array): The labels corresponding to the full training data.\n",
    "    index (faiss.IndexFlatL2): The FAISS index used for clustering and searching.\n",
    "    X_test_normalized (numpy array): The normalized test data embeddings.\n",
    "    y_test (numpy array): The true labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    None: The function prints the test accuracy after each iteration and the final classification report.\n",
    "    \"\"\"\n",
    "    # Active Learning Loop\n",
    "    for iteration in range(iterations):\n",
    "        # Adjust the number of clusters dynamically\n",
    "        \n",
    "        num_clusters = dynamic_clusters + iteration  # Increase clusters gradually\n",
    "\n",
    "        # Initialize FAISS clustering\n",
    "        clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "        clustering.verbose = False\n",
    "        clustering.niter = 50\n",
    "\n",
    "        # Convert remaining indices to the appropriate format\n",
    "        remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "        index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "        clustering.train(remaining_data, index_flat)\n",
    "\n",
    "        # Get cluster assignments\n",
    "        D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "        \n",
    "        # Convert FAISS centroids to numpy array\n",
    "        centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "        # Select samples from each cluster using a refined strategy\n",
    "        selected_indices = []\n",
    "        for cluster in range(num_clusters):\n",
    "            cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "            if cluster_indices:\n",
    "                # Select the most uncertain samples within each cluster\n",
    "                cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "                probs = clf.predict_proba(cluster_data)\n",
    "                uncertainty = 1 - np.max(probs, axis=1)\n",
    "\n",
    "                # Calculate diversity by distance from the centroid\n",
    "                distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "                combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "                # Prioritize samples with high uncertainty and diversity\n",
    "                sorted_indices = np.argsort(-combined_scores)\n",
    "                num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "                selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "                selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "        # Ensure unique samples and limit to the budget\n",
    "        selected_indices = list(set(selected_indices))\n",
    "        if len(selected_indices) > budget_per_iteration:\n",
    "            selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "        # Add selected samples to the training set\n",
    "        X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "        y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "        # Remove selected samples from the pool\n",
    "        remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "        # Update FAISS Index with new training data\n",
    "        index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "        # Incrementally train the classifier with new samples\n",
    "        clf.partial_fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the classifier on the test set after each iteration\n",
    "        y_pred = clf.predict(X_test_normalized)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, X_train, y_train, optimizer, criterion, batch_size=64, epochs=5):\n",
    "    \"\"\"\n",
    "    Train a neural network model using the provided training data, optimizer, and loss function.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The neural network model to be trained.\n",
    "    X_train (torch.Tensor): The input training data.\n",
    "    y_train (torch.Tensor): The labels corresponding to the training data.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "    criterion (torch.nn.Module): The loss function used to compute the error between predictions and true labels.\n",
    "    batch_size (int, optional): The number of samples per batch during training. Default is 64.\n",
    "    epochs (int, optional): The number of times to iterate over the entire training dataset. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    None: The function prints the loss after each epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(device, model, X_test, batch_size=64):\n",
    "    \"\"\"\n",
    "    Evaluate a neural network model on the test dataset and return the predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    device (torch.device): The device (CPU or GPU) on which to perform the evaluation.\n",
    "    model (torch.nn.Module): The neural network model to be evaluated.\n",
    "    X_test (numpy array): The test data on which to evaluate the model.\n",
    "    batch_size (int, optional): The number of samples per batch during evaluation. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "    numpy array: An array of predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert data to PyTorch tensor\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_test_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            outputs = model(inputs[0])\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AL_with_nn(iterations,dynamic_clusters,embedding_dim,X_train_full_normalized,device,model,uncertainty_threshold,budget_per_iteration,y_train_full,optimizer, criterion,X_test_normalized,y_test):\n",
    "    \"\"\"\n",
    "    Perform active learning with neural networks using dynamic clustering and uncertainty sampling.\n",
    "\n",
    "    Parameters:\n",
    "    iterations (int): The number of active learning iterations to perform.\n",
    "    dynamic_clusters (int): The initial number of clusters, which increases with each iteration.\n",
    "    embedding_dim (int): The dimensionality of the embedding space.\n",
    "    X_train_full_normalized (numpy array): The normalized full training data embeddings.\n",
    "    device (torch.device): The device (CPU or GPU) on which to perform computations.\n",
    "    model (torch.nn.Module): The neural network model to be trained.\n",
    "    uncertainty_threshold (float): The threshold for selecting uncertain samples within each cluster.\n",
    "    budget_per_iteration (int): The maximum number of samples to be added to the training set per iteration.\n",
    "    y_train_full (numpy array): The labels corresponding to the full training data.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "    criterion (torch.nn.Module): The loss function used to compute the error between predictions and true labels.\n",
    "    X_test_normalized (numpy array): The normalized test data embeddings.\n",
    "    y_test (numpy array): The true labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    None: The function prints the test accuracy after each iteration and the final classification report.\n",
    "    \"\"\"\n",
    "    # Active Learning Loop\n",
    "    for iteration in range(iterations):\n",
    "        # Adjust the number of clusters dynamically\n",
    "        num_clusters = min(dynamic_clusters + iteration, len(remaining_indices) // 2)\n",
    "\n",
    "        # Initialize FAISS clustering\n",
    "        clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "        clustering.verbose = False\n",
    "        clustering.niter = 50\n",
    "\n",
    "        # Convert remaining indices to the appropriate format\n",
    "        remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "        index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "        clustering.train(remaining_data, index_flat)\n",
    "\n",
    "        # Get cluster assignments\n",
    "        _, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "        cluster_assignments = cluster_assignments.flatten()\n",
    "\n",
    "        # Convert FAISS centroids to numpy array\n",
    "        centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "        # Select samples from each cluster using a refined strategy\n",
    "        selected_indices = []\n",
    "        for cluster in range(num_clusters):\n",
    "            cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "            if cluster_indices:\n",
    "                # Select the most uncertain samples within each cluster\n",
    "                cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "                cluster_data_tensor = torch.tensor(cluster_data, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Get model predictions and uncertainties\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(cluster_data_tensor)\n",
    "                    probs = nn.functional.softmax(outputs, dim=1)\n",
    "                    uncertainty = 1 - torch.max(probs, dim=1).values.cpu().numpy()\n",
    "\n",
    "                # Calculate diversity by distance from the centroid\n",
    "                distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "                combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "                # Prioritize samples with high uncertainty and diversity\n",
    "                sorted_indices = np.argsort(-combined_scores)\n",
    "                num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "                selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "                selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "        # Ensure unique samples and limit to the budget\n",
    "        selected_indices = list(set(selected_indices))\n",
    "        if len(selected_indices) > budget_per_iteration:\n",
    "            selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "        # Add selected samples to the training set\n",
    "        X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "        y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "        # Remove selected samples from the pool\n",
    "        remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "        # Update FAISS Index with new training data\n",
    "        index_flat.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "        # Convert updated training data to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "        # Incrementally train the neural network with the new samples\n",
    "        train_nn(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size=32, epochs=5)\n",
    "\n",
    "        # Evaluate the neural network on the test set after each iteration\n",
    "        X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "        y_pred = evaluate_nn(device,model, X_test_tensor)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "    y_pred_final = evaluate_nn(device,model, X_test_normalized)\n",
    "    final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"Final Test Accuracy after all iterations: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,X_test, X_train_full_normalized,X_test_normalized,y_train_full,y_test = load_data('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 762/762 [01:04<00:00, 11.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Active Learning parameters\n",
    "initial_train_size = 100\n",
    "iterations = 20\n",
    "sample_size = 250\n",
    "uncertainty_threshold = 0.2\n",
    "budget_per_iteration = 250    \n",
    "\n",
    "#Selecting initial training set randomly\n",
    "np.random.seed(42)\n",
    "pool_indices = np.random.choice(len(X_train_full_normalized), initial_train_size, replace=False)\n",
    "X_train = X_train_full_normalized[pool_indices]\n",
    "y_train = np.array(y_train_full)[pool_indices]\n",
    "\n",
    "# Initialize the FAISS Index for Cosine Similarity\n",
    "faiss.omp_set_num_threads(12)\n",
    "embedding_dim = X_train_full_normalized.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Inner product index for cosine similarity\n",
    "index.add(X_train_full_normalized)  # Add all normalized vectors to the index\n",
    "\n",
    "# Remaining pool of indices\n",
    "remaining_indices = list(set(range(len(X_train_full_normalized))) - set(pool_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = SGDClassifier(loss='log_loss', random_state=42)\n",
    "clf.partial_fit(X_train, y_train, classes=np.unique(y_train_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool, X_train, y_pool, y_train = train_test_split(X_train_full, y_train_full, test_size=0.00255, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Accuracy = 0.3765\n",
      "Iteration 2: Accuracy = 0.4044\n",
      "Iteration 3: Accuracy = 0.4874\n",
      "Iteration 4: Accuracy = 0.5798\n",
      "Iteration 5: Accuracy = 0.5769\n",
      "Iteration 6: Accuracy = 0.5820\n",
      "Iteration 7: Accuracy = 0.6081\n",
      "Iteration 8: Accuracy = 0.6122\n",
      "Iteration 9: Accuracy = 0.6225\n",
      "Iteration 10: Accuracy = 0.6138\n",
      "Iteration 11: Accuracy = 0.6218\n",
      "Iteration 12: Accuracy = 0.6007\n",
      "Iteration 13: Accuracy = 0.6282\n",
      "Iteration 14: Accuracy = 0.6323\n",
      "Iteration 15: Accuracy = 0.6286\n",
      "Iteration 16: Accuracy = 0.6274\n",
      "Iteration 17: Accuracy = 0.6366\n",
      "Iteration 18: Accuracy = 0.6354\n",
      "Iteration 19: Accuracy = 0.6327\n",
      "Iteration 20: Accuracy = 0.6245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.39      0.41       134\n",
      "           1       0.40      0.09      0.14        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.53      0.74      0.62      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.72      0.70      0.71      2233\n",
      "           7       1.00      0.02      0.04        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.63      0.69      0.66       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.86      0.33      0.47        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.27      0.08      0.13       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.76      0.67      0.71       128\n",
      "\n",
      "    accuracy                           0.62      4871\n",
      "   macro avg       0.28      0.19      0.19      4871\n",
      "weighted avg       0.60      0.62      0.60      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "random_sampling(iterations,clf,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN As Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.35      0.42       134\n",
      "           1       0.57      0.09      0.15        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.58      0.69      0.63      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.69      0.78      0.73      2233\n",
      "           7       0.67      0.04      0.07        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.65      0.67      0.66       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.33      0.01      0.02        80\n",
      "          15       0.89      0.29      0.44        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.29      0.09      0.14       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.84      0.65      0.73       128\n",
      "\n",
      "    accuracy                           0.64      4871\n",
      "   macro avg       0.30      0.18      0.20      4871\n",
      "weighted avg       0.61      0.64      0.61      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "dynamic_clusters = 5\n",
    "\n",
    "ANN_as_clustering(iterations,dynamic_clusters,embedding_dim,X_train_full_normalized,clf,budget_per_iteration,uncertainty_threshold,y_train_full,index,X_test_normalized,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super (NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train_full_normalized.shape[1]\n",
    "hidden_size = 128  # You can adjust the number of neurons\n",
    "num_classes = len(np.unique(y_train_full))\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "model = NN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 5 centroids: please provide at least 195 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.9518\n",
      "Epoch [2/5], Loss: 2.9193\n",
      "Epoch [3/5], Loss: 2.8840\n",
      "Epoch [4/5], Loss: 2.8416\n",
      "Epoch [5/5], Loss: 2.8002\n",
      "Iteration 1: Test Accuracy = 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 6 centroids: please provide at least 234 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.7380\n",
      "Epoch [2/5], Loss: 2.6956\n",
      "Epoch [3/5], Loss: 2.6078\n",
      "Epoch [4/5], Loss: 2.5587\n",
      "Epoch [5/5], Loss: 2.4624\n",
      "Iteration 2: Test Accuracy = 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3665\n",
      "Epoch [2/5], Loss: 2.2774\n",
      "Epoch [3/5], Loss: 2.2254\n",
      "Epoch [4/5], Loss: 2.1046\n",
      "Epoch [5/5], Loss: 2.0336\n",
      "Iteration 3: Test Accuracy = 0.3837\n",
      "Epoch [1/5], Loss: 1.9457\n",
      "Epoch [2/5], Loss: 1.7915\n",
      "Epoch [3/5], Loss: 1.7427\n",
      "Epoch [4/5], Loss: 1.6912\n",
      "Epoch [5/5], Loss: 1.6481\n",
      "Iteration 4: Test Accuracy = 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.5706\n",
      "Epoch [2/5], Loss: 1.4492\n",
      "Epoch [3/5], Loss: 1.5778\n",
      "Epoch [4/5], Loss: 1.4310\n",
      "Epoch [5/5], Loss: 1.3219\n",
      "Iteration 5: Test Accuracy = 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3532\n",
      "Epoch [2/5], Loss: 1.3258\n",
      "Epoch [3/5], Loss: 1.2461\n",
      "Epoch [4/5], Loss: 1.2282\n",
      "Epoch [5/5], Loss: 1.1023\n",
      "Iteration 6: Test Accuracy = 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.2506\n",
      "Epoch [2/5], Loss: 1.1766\n",
      "Epoch [3/5], Loss: 1.1076\n",
      "Epoch [4/5], Loss: 1.0224\n",
      "Epoch [5/5], Loss: 0.9852\n",
      "Iteration 7: Test Accuracy = 0.5048\n",
      "Epoch [1/5], Loss: 1.0508\n",
      "Epoch [2/5], Loss: 0.9411\n",
      "Epoch [3/5], Loss: 0.8931\n",
      "Epoch [4/5], Loss: 0.9006\n",
      "Epoch [5/5], Loss: 0.8365\n",
      "Iteration 8: Test Accuracy = 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8622\n",
      "Epoch [2/5], Loss: 0.7847\n",
      "Epoch [3/5], Loss: 0.7749\n",
      "Epoch [4/5], Loss: 0.6680\n",
      "Epoch [5/5], Loss: 0.6938\n",
      "Iteration 9: Test Accuracy = 0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6175\n",
      "Epoch [2/5], Loss: 0.6706\n",
      "Epoch [3/5], Loss: 0.6131\n",
      "Epoch [4/5], Loss: 0.6128\n",
      "Epoch [5/5], Loss: 0.5380\n",
      "Iteration 10: Test Accuracy = 0.5307\n",
      "Epoch [1/5], Loss: 0.5111\n",
      "Epoch [2/5], Loss: 0.5088\n",
      "Epoch [3/5], Loss: 0.5019\n",
      "Epoch [4/5], Loss: 0.4458\n",
      "Epoch [5/5], Loss: 0.4143\n",
      "Iteration 11: Test Accuracy = 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4124\n",
      "Epoch [2/5], Loss: 0.3874\n",
      "Epoch [3/5], Loss: 0.3813\n",
      "Epoch [4/5], Loss: 0.3685\n",
      "Epoch [5/5], Loss: 0.3462\n",
      "Iteration 12: Test Accuracy = 0.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3186\n",
      "Epoch [2/5], Loss: 0.3059\n",
      "Epoch [3/5], Loss: 0.3153\n",
      "Epoch [4/5], Loss: 0.2884\n",
      "Epoch [5/5], Loss: 0.2595\n",
      "Iteration 13: Test Accuracy = 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2616\n",
      "Epoch [2/5], Loss: 0.2304\n",
      "Epoch [3/5], Loss: 0.2362\n",
      "Epoch [4/5], Loss: 0.2133\n",
      "Epoch [5/5], Loss: 0.2026\n",
      "Iteration 14: Test Accuracy = 0.5317\n",
      "Epoch [1/5], Loss: 0.1929\n",
      "Epoch [2/5], Loss: 0.1895\n",
      "Epoch [3/5], Loss: 0.1675\n",
      "Epoch [4/5], Loss: 0.1728\n",
      "Epoch [5/5], Loss: 0.1577\n",
      "Iteration 15: Test Accuracy = 0.5323\n",
      "Epoch [1/5], Loss: 0.1561\n",
      "Epoch [2/5], Loss: 0.1475\n",
      "Epoch [3/5], Loss: 0.1478\n",
      "Epoch [4/5], Loss: 0.1298\n",
      "Epoch [5/5], Loss: 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: Test Accuracy = 0.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1240\n",
      "Epoch [2/5], Loss: 0.1196\n",
      "Epoch [3/5], Loss: 0.1121\n",
      "Epoch [4/5], Loss: 0.1080\n",
      "Epoch [5/5], Loss: 0.1066\n",
      "Iteration 17: Test Accuracy = 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1051\n",
      "Epoch [2/5], Loss: 0.1019\n",
      "Epoch [3/5], Loss: 0.0967\n",
      "Epoch [4/5], Loss: 0.0986\n",
      "Epoch [5/5], Loss: 0.0923\n",
      "Iteration 18: Test Accuracy = 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0934\n",
      "Epoch [2/5], Loss: 0.0887\n",
      "Epoch [3/5], Loss: 0.0801\n",
      "Epoch [4/5], Loss: 0.0792\n",
      "Epoch [5/5], Loss: 0.0731\n",
      "Iteration 19: Test Accuracy = 0.5332\n",
      "Epoch [1/5], Loss: 0.0720\n",
      "Epoch [2/5], Loss: 0.0711\n",
      "Epoch [3/5], Loss: 0.0700\n",
      "Epoch [4/5], Loss: 0.0684\n",
      "Epoch [5/5], Loss: 0.0666\n",
      "Iteration 20: Test Accuracy = 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "AL_with_nn(iterations,dynamic_clusters,embedding_dim,X_train_full_normalized,device,model,uncertainty_threshold,budget_per_iteration,y_train_full,optimizer,criterion,X_test_normalized,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysisvisualizationfiles-qa5NmZKI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
