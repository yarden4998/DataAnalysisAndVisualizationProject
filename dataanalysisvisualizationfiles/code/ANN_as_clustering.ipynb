{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score,classification_report,pairwise_distances_argmin_min,silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from preprocess_data import preprocess_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances,pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = preprocess_data()\n",
    "\n",
    "data['num_genres'] = data['genre'].apply(len)\n",
    "data = data[data['num_genres'] == 1]\n",
    "\n",
    "descriptions = data['description_processed'].tolist()\n",
    "genres = data['genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Encode the genres as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(genres)\n",
    "\n",
    "# Load a pre-trained sentence-transformer model to convert text to embeddings\n",
    "#model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 762/762 [01:05<00:00, 11.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert descriptions to vector embeddings\n",
    "X = model.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the embeddings for cosine similarity\n",
    "X_train_full_normalized = normalize(X_train_full, axis=1, norm='l2')\n",
    "X_test_normalized = normalize(X_test, axis=1, norm='l2')\n",
    "\n",
    "# Active Learning parameters\n",
    "initial_train_size = 50 \n",
    "iterations = 20\n",
    "sample_size = 1000\n",
    "uncertainty_threshold = 0.2\n",
    "budget_per_iteration = 500    \n",
    "\n",
    "#Selecting initial training set randomly\n",
    "np.random.seed(42)\n",
    "pool_indices = np.random.choice(len(X_train_full_normalized), initial_train_size, replace=False)\n",
    "X_train = X_train_full_normalized[pool_indices]\n",
    "y_train = np.array(y_train_full)[pool_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the FAISS Index for Cosine Similarity\n",
    "faiss.omp_set_num_threads(12)\n",
    "embedding_dim = X_train_full_normalized.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Inner product index for cosine similarity\n",
    "index.add(X_train_full_normalized)  # Add all normalized vectors to the index\n",
    "\n",
    "# Remaining pool of indices\n",
    "remaining_indices = list(set(range(len(X_train_full_normalized))) - set(pool_indices))\n",
    "\n",
    "# Initialize the classifier\n",
    "#clf = RandomForestClassifier(random_state=42)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=42)\n",
    "clf.partial_fit(X_train, y_train, classes=np.unique(y_train_full))\n",
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool, X_train, y_pool, y_train = train_test_split(X_train_full, y_train_full, test_size=0.00255, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Accuracy = 0.2650\n",
      "Iteration 2: Accuracy = 0.5822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Accuracy = 0.5449\n",
      "Iteration 4: Accuracy = 0.5366\n",
      "Iteration 5: Accuracy = 0.6241\n",
      "Iteration 6: Accuracy = 0.6358\n",
      "Iteration 7: Accuracy = 0.6374\n",
      "Iteration 8: Accuracy = 0.6430\n",
      "Iteration 9: Accuracy = 0.6446\n",
      "Iteration 10: Accuracy = 0.6303\n",
      "Iteration 11: Accuracy = 0.6455\n",
      "Iteration 12: Accuracy = 0.6428\n",
      "Iteration 13: Accuracy = 0.6444\n",
      "Iteration 14: Accuracy = 0.6506\n",
      "Iteration 15: Accuracy = 0.6475\n",
      "Iteration 16: Accuracy = 0.6485\n",
      "Iteration 17: Accuracy = 0.6430\n",
      "Iteration 18: Accuracy = 0.6422\n",
      "Iteration 19: Accuracy = 0.6504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.36      0.44       134\n",
      "           1       0.60      0.06      0.12        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.63      0.60      0.62      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.65      0.86      0.74      2233\n",
      "           7       0.50      0.02      0.04        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.72      0.63      0.67       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.85      0.31      0.45        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.38      0.05      0.09       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.85      0.68      0.76       128\n",
      "\n",
      "    accuracy                           0.65      4871\n",
      "   macro avg       0.29      0.18      0.20      4871\n",
      "weighted avg       0.61      0.65      0.61      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Incrementally train the classifier with the current training set\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate performance on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}: Accuracy = {accuracy_scores[-1]:.4f}\")\n",
    "\n",
    "    # Random sampling of new data points\n",
    "    sample_size = min(sample_size, len(X_pool))\n",
    "    selected_indices = np.random.choice(len(X_pool), size=sample_size, replace=False)\n",
    "    X_sample = X_pool[selected_indices]\n",
    "\n",
    "    # Handle potential differences in data type or format\n",
    "    try:\n",
    "        y_sample = y_pool[selected_indices]\n",
    "    except:\n",
    "        y_sample = y_pool.iloc[selected_indices]\n",
    "    \n",
    "    # Update the training set with new samples\n",
    "    X_train = np.vstack([X_train, X_sample])\n",
    "    y_train = np.concatenate([y_train, y_sample])\n",
    "\n",
    "    # Remove the selected samples from the pool\n",
    "    X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "    # Stop criteria based on the pool size or maximum training size\n",
    "    if len(X_pool) == 0 or len(X_pool) < sample_size or len(X_train) >= 60000:\n",
    "        break\n",
    "\n",
    "# Print the final classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN As Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.5221\n",
      "Iteration 2: Test Accuracy = 0.5272\n",
      "Iteration 3: Test Accuracy = 0.5847\n",
      "Iteration 4: Test Accuracy = 0.4927\n",
      "Iteration 5: Test Accuracy = 0.5995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5349/3731193401.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Convert remaining indices to the appropriate format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mremaining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_full_normalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mindex_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlatL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize FAISS index for clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Get cluster assignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_assignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, index, weights)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x, index, x_weights)\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0mindex\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0massignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mweight\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msize\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \"\"\"\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClustering_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Define number of clusters\n",
    "    num_clusters = min(int(np.sqrt(len(remaining_indices))), len(remaining_indices))\n",
    "    \n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50  # Number of iterations for clustering\n",
    "    \n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)  # Initialize FAISS index for clustering\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "    # Select samples from each cluster using a hybrid strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Find the closest sample to the cluster center\n",
    "            cluster_center = centroids[cluster].reshape(1, -1)\n",
    "            distances, _ = index.search(cluster_center, len(cluster_indices))\n",
    "            closest_sample_index = cluster_indices[distances[0].argmin()]\n",
    "            selected_indices.append(closest_sample_index)\n",
    "\n",
    "            # Diversity Sampling: Select most diverse samples in the cluster\n",
    "            cluster_data = X_train_full_normalized[cluster_indices]\n",
    "            pairwise_distances_matrix = pairwise_distances(cluster_data)\n",
    "            diversity_scores = pairwise_distances_matrix.mean(axis=1)\n",
    "            most_diverse_index = cluster_indices[np.argmax(diversity_scores)]\n",
    "            selected_indices.append(most_diverse_index)\n",
    "\n",
    "    # Uncertainty sampling\n",
    "    if len(remaining_indices) > 0:\n",
    "        # Predict probabilities for remaining samples\n",
    "        probs = clf.predict_proba(X_train_full_normalized[remaining_indices])\n",
    "        # Calculate uncertainty as 1 - max probability\n",
    "        uncertainty = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Determine the number of uncertain samples to select\n",
    "        num_uncertain_samples = int(sample_size * uncertainty_threshold)\n",
    "        if num_uncertain_samples > len(remaining_indices):\n",
    "            num_uncertain_samples = len(remaining_indices)\n",
    "        \n",
    "        # Select the most uncertain samples\n",
    "        uncertain_indices = np.argsort(-uncertainty)[:num_uncertain_samples]\n",
    "        \n",
    "        # Add uncertain samples to selected indices\n",
    "        selected_indices.extend([remaining_indices[i] for i in uncertain_indices])\n",
    "\n",
    "    # Ensure selected indices are unique and limited to the budget per iteration\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "    #print(np.unique(y_train))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.19      0.29       134\n",
      "           1       0.58      0.15      0.24        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.62      0.47      0.53      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.61      0.88      0.72      2233\n",
      "           7       1.00      0.02      0.04        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.73      0.50      0.59       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.71      0.40      0.51        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.32      0.14      0.20       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.83      0.66      0.74       128\n",
      "\n",
      "    accuracy                           0.61      4871\n",
      "   macro avg       0.30      0.17      0.19      4871\n",
      "weighted avg       0.59      0.61      0.57      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN As Clustering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_focus_ratio = 0.3 \n",
    "dynamic_clusters = 5\n",
    "# Define a threshold to identify minority classes\n",
    "minority_threshold = 100  # You can adjust this threshold based on your dataset\n",
    "\n",
    "# Calculate the original class distribution\n",
    "original_class_distribution = np.bincount(y_train_full)\n",
    "\n",
    "# Identify minority classes based on the threshold\n",
    "minority_classes = [i for i, count in enumerate(original_class_distribution) if count < minority_threshold]\n",
    "# Initialize the dictionary to store class distribution for each cluster\n",
    "cluster_class_distribution = {i: {} for i in range(num_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class Distribution:\n",
      "Class 0: 539 samples\n",
      "Class 1: 164 samples\n",
      "Class 2: 71 samples\n",
      "Class 3: 38 samples\n",
      "Class 4: 5314 samples\n",
      "Class 5: 106 samples\n",
      "Class 6: 9081 samples\n",
      "Class 7: 210 samples\n",
      "Class 8: 56 samples\n",
      "Class 9: 21 samples\n",
      "Class 10: 1753 samples\n",
      "Class 11: 25 samples\n",
      "Class 12: 97 samples\n",
      "Class 13: 105 samples\n",
      "Class 14: 299 samples\n",
      "Class 15: 189 samples\n",
      "Class 16: 12 samples\n",
      "Class 17: 895 samples\n",
      "Class 18: 41 samples\n",
      "Class 19: 466 samples\n"
     ]
    }
   ],
   "source": [
    "original_class_distribution = np.bincount(y_train_full)\n",
    "print(\"Original Class Distribution:\")\n",
    "for class_label, count in enumerate(original_class_distribution):\n",
    "    print(f\"Class {class_label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.4931\n",
      "Iteration 2: Test Accuracy = 0.5204\n",
      "Iteration 3: Test Accuracy = 0.6009\n",
      "Iteration 4: Test Accuracy = 0.5878\n",
      "Iteration 5: Test Accuracy = 0.5968\n",
      "Iteration 6: Test Accuracy = 0.5939\n",
      "Iteration 7: Test Accuracy = 0.6335\n",
      "Iteration 8: Test Accuracy = 0.6243\n",
      "Iteration 9: Test Accuracy = 0.6325\n",
      "Iteration 10: Test Accuracy = 0.6370\n",
      "Iteration 11: Test Accuracy = 0.6216\n",
      "Iteration 12: Test Accuracy = 0.6405\n",
      "Iteration 13: Test Accuracy = 0.6112\n",
      "Iteration 14: Test Accuracy = 0.6416\n",
      "Iteration 15: Test Accuracy = 0.6483\n",
      "Iteration 16: Test Accuracy = 0.6455\n",
      "Iteration 17: Test Accuracy = 0.6413\n",
      "Iteration 18: Test Accuracy = 0.6465\n",
      "Iteration 19: Test Accuracy = 0.6463\n",
      "Iteration 20: Test Accuracy = 0.6446\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Adjust the number of clusters dynamically\n",
    "    \n",
    "    num_clusters = dynamic_clusters + iteration  # Increase clusters gradually\n",
    "\n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50\n",
    "\n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "    \n",
    "    # Print class distribution within each cluster\n",
    "    # print(\"\\nClass distribution within each cluster:\")\n",
    "    # for cluster in range(num_clusters):\n",
    "    #     cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "    #     if cluster_indices:\n",
    "    #         cluster_labels = [y_train_full[i] for i in cluster_indices]\n",
    "    #         unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    #         cluster_distribution = dict(zip(unique, counts))\n",
    "    #         print(f\"Cluster {cluster}: {cluster_distribution}\")\n",
    "\n",
    "    # Select samples from each cluster using a refined strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Select the most uncertain samples within each cluster\n",
    "            cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "            probs = clf.predict_proba(cluster_data)\n",
    "            uncertainty = 1 - np.max(probs, axis=1)\n",
    "\n",
    "            # Calculate diversity by distance from the centroid\n",
    "            distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "            combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "            # Prioritize samples with high uncertainty and diversity\n",
    "            sorted_indices = np.argsort(-combined_scores)\n",
    "            num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "            selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "            selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "    # Ensure unique samples and limit to the budget\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.35      0.42       134\n",
      "           1       0.57      0.09      0.15        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.58      0.69      0.63      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.69      0.78      0.73      2233\n",
      "           7       0.67      0.04      0.07        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.65      0.67      0.66       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.33      0.01      0.02        80\n",
      "          15       0.89      0.29      0.44        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.29      0.09      0.14       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.84      0.65      0.73       128\n",
      "\n",
      "    accuracy                           0.64      4871\n",
      "   macro avg       0.30      0.18      0.20      4871\n",
      "weighted avg       0.61      0.64      0.61      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super (NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train_full_normalized.shape[1]\n",
    "hidden_size = 128  # You can adjust the number of neurons\n",
    "num_classes = len(np.unique(y_train_full))\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "model = NN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, X_train, y_train, optimizer, criterion, batch_size=64, epochs=5):\n",
    "    model.train()\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(model, X_test, batch_size=64):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert data to PyTorch tensor\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_test_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            outputs = model(inputs[0])\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 5 centroids: please provide at least 195 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.9518\n",
      "Epoch [2/5], Loss: 2.9193\n",
      "Epoch [3/5], Loss: 2.8840\n",
      "Epoch [4/5], Loss: 2.8416\n",
      "Epoch [5/5], Loss: 2.8002\n",
      "Iteration 1: Test Accuracy = 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 6 centroids: please provide at least 234 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.7380\n",
      "Epoch [2/5], Loss: 2.6956\n",
      "Epoch [3/5], Loss: 2.6078\n",
      "Epoch [4/5], Loss: 2.5587\n",
      "Epoch [5/5], Loss: 2.4624\n",
      "Iteration 2: Test Accuracy = 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3665\n",
      "Epoch [2/5], Loss: 2.2774\n",
      "Epoch [3/5], Loss: 2.2254\n",
      "Epoch [4/5], Loss: 2.1046\n",
      "Epoch [5/5], Loss: 2.0336\n",
      "Iteration 3: Test Accuracy = 0.3837\n",
      "Epoch [1/5], Loss: 1.9457\n",
      "Epoch [2/5], Loss: 1.7915\n",
      "Epoch [3/5], Loss: 1.7427\n",
      "Epoch [4/5], Loss: 1.6912\n",
      "Epoch [5/5], Loss: 1.6481\n",
      "Iteration 4: Test Accuracy = 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.5706\n",
      "Epoch [2/5], Loss: 1.4492\n",
      "Epoch [3/5], Loss: 1.5778\n",
      "Epoch [4/5], Loss: 1.4310\n",
      "Epoch [5/5], Loss: 1.3219\n",
      "Iteration 5: Test Accuracy = 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3532\n",
      "Epoch [2/5], Loss: 1.3258\n",
      "Epoch [3/5], Loss: 1.2461\n",
      "Epoch [4/5], Loss: 1.2282\n",
      "Epoch [5/5], Loss: 1.1023\n",
      "Iteration 6: Test Accuracy = 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.2506\n",
      "Epoch [2/5], Loss: 1.1766\n",
      "Epoch [3/5], Loss: 1.1076\n",
      "Epoch [4/5], Loss: 1.0224\n",
      "Epoch [5/5], Loss: 0.9852\n",
      "Iteration 7: Test Accuracy = 0.5048\n",
      "Epoch [1/5], Loss: 1.0508\n",
      "Epoch [2/5], Loss: 0.9411\n",
      "Epoch [3/5], Loss: 0.8931\n",
      "Epoch [4/5], Loss: 0.9006\n",
      "Epoch [5/5], Loss: 0.8365\n",
      "Iteration 8: Test Accuracy = 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8622\n",
      "Epoch [2/5], Loss: 0.7847\n",
      "Epoch [3/5], Loss: 0.7749\n",
      "Epoch [4/5], Loss: 0.6680\n",
      "Epoch [5/5], Loss: 0.6938\n",
      "Iteration 9: Test Accuracy = 0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6175\n",
      "Epoch [2/5], Loss: 0.6706\n",
      "Epoch [3/5], Loss: 0.6131\n",
      "Epoch [4/5], Loss: 0.6128\n",
      "Epoch [5/5], Loss: 0.5380\n",
      "Iteration 10: Test Accuracy = 0.5307\n",
      "Epoch [1/5], Loss: 0.5111\n",
      "Epoch [2/5], Loss: 0.5088\n",
      "Epoch [3/5], Loss: 0.5019\n",
      "Epoch [4/5], Loss: 0.4458\n",
      "Epoch [5/5], Loss: 0.4143\n",
      "Iteration 11: Test Accuracy = 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4124\n",
      "Epoch [2/5], Loss: 0.3874\n",
      "Epoch [3/5], Loss: 0.3813\n",
      "Epoch [4/5], Loss: 0.3685\n",
      "Epoch [5/5], Loss: 0.3462\n",
      "Iteration 12: Test Accuracy = 0.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3186\n",
      "Epoch [2/5], Loss: 0.3059\n",
      "Epoch [3/5], Loss: 0.3153\n",
      "Epoch [4/5], Loss: 0.2884\n",
      "Epoch [5/5], Loss: 0.2595\n",
      "Iteration 13: Test Accuracy = 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2616\n",
      "Epoch [2/5], Loss: 0.2304\n",
      "Epoch [3/5], Loss: 0.2362\n",
      "Epoch [4/5], Loss: 0.2133\n",
      "Epoch [5/5], Loss: 0.2026\n",
      "Iteration 14: Test Accuracy = 0.5317\n",
      "Epoch [1/5], Loss: 0.1929\n",
      "Epoch [2/5], Loss: 0.1895\n",
      "Epoch [3/5], Loss: 0.1675\n",
      "Epoch [4/5], Loss: 0.1728\n",
      "Epoch [5/5], Loss: 0.1577\n",
      "Iteration 15: Test Accuracy = 0.5323\n",
      "Epoch [1/5], Loss: 0.1561\n",
      "Epoch [2/5], Loss: 0.1475\n",
      "Epoch [3/5], Loss: 0.1478\n",
      "Epoch [4/5], Loss: 0.1298\n",
      "Epoch [5/5], Loss: 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: Test Accuracy = 0.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1240\n",
      "Epoch [2/5], Loss: 0.1196\n",
      "Epoch [3/5], Loss: 0.1121\n",
      "Epoch [4/5], Loss: 0.1080\n",
      "Epoch [5/5], Loss: 0.1066\n",
      "Iteration 17: Test Accuracy = 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1051\n",
      "Epoch [2/5], Loss: 0.1019\n",
      "Epoch [3/5], Loss: 0.0967\n",
      "Epoch [4/5], Loss: 0.0986\n",
      "Epoch [5/5], Loss: 0.0923\n",
      "Iteration 18: Test Accuracy = 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0934\n",
      "Epoch [2/5], Loss: 0.0887\n",
      "Epoch [3/5], Loss: 0.0801\n",
      "Epoch [4/5], Loss: 0.0792\n",
      "Epoch [5/5], Loss: 0.0731\n",
      "Iteration 19: Test Accuracy = 0.5332\n",
      "Epoch [1/5], Loss: 0.0720\n",
      "Epoch [2/5], Loss: 0.0711\n",
      "Epoch [3/5], Loss: 0.0700\n",
      "Epoch [4/5], Loss: 0.0684\n",
      "Epoch [5/5], Loss: 0.0666\n",
      "Iteration 20: Test Accuracy = 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Adjust the number of clusters dynamically\n",
    "    num_clusters = min(dynamic_clusters + iteration, len(remaining_indices) // 2)\n",
    "\n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50\n",
    "\n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    _, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    cluster_assignments = cluster_assignments.flatten()\n",
    "\n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "    # Select samples from each cluster using a refined strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Select the most uncertain samples within each cluster\n",
    "            cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "            cluster_data_tensor = torch.tensor(cluster_data, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Get model predictions and uncertainties\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(cluster_data_tensor)\n",
    "                probs = nn.functional.softmax(outputs, dim=1)\n",
    "                uncertainty = 1 - torch.max(probs, dim=1).values.cpu().numpy()\n",
    "\n",
    "            # Calculate diversity by distance from the centroid\n",
    "            distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "            combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "            # Prioritize samples with high uncertainty and diversity\n",
    "            sorted_indices = np.argsort(-combined_scores)\n",
    "            num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "            selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "            selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "    # Ensure unique samples and limit to the budget\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index_flat.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Convert updated training data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    # Incrementally train the neural network with the new samples\n",
    "    train_nn(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size=32, epochs=5)\n",
    "\n",
    "    # Evaluate the neural network on the test set after each iteration\n",
    "    X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "    y_pred = evaluate_nn(model, X_test_tensor)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy after all iterations: 0.5325\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = evaluate_nn(model, X_test_normalized)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "print(f\"Final Test Accuracy after all iterations: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.9887\n",
      "Epoch [2/5], Loss: 2.9415\n",
      "Epoch [3/5], Loss: 2.9004\n",
      "Epoch [4/5], Loss: 2.8694\n",
      "Epoch [5/5], Loss: 2.8125\n",
      "Iteration 1: Accuracy = 0.5182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       134\n",
      "           1       0.00      0.00      0.00        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.49      0.52      0.50      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.53      0.81      0.64      2233\n",
      "           7       0.00      0.00      0.00        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.00      0.00      0.00        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00       128\n",
      "\n",
      "    accuracy                           0.52      4871\n",
      "   macro avg       0.05      0.07      0.06      4871\n",
      "weighted avg       0.38      0.52      0.44      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "accuracy_scores = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Incrementally train the neural network with the current training set\n",
    "    train_nn(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size=32, epochs=5)\n",
    "    \n",
    "    # Predict and evaluate performance on the test set\n",
    "    y_pred = evaluate_nn(model, X_test_tensor)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}: Accuracy = {accuracy_scores[-1]:.4f}\")\n",
    "\n",
    "    # Predict probabilities on the pool data to measure uncertainty\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_pool_tensor = torch.tensor(X_pool, dtype=torch.float32).to(device)\n",
    "        pool_probs = nn.functional.softmax(model(X_pool_tensor), dim=1)\n",
    "        uncertainty = 1 - torch.max(pool_probs, dim=1).values.cpu().numpy()\n",
    "\n",
    "    # Select samples based on uncertainty\n",
    "    sample_size = min(sample_size, len(X_pool))\n",
    "    selected_indices = np.argsort(-uncertainty)[:sample_size]\n",
    "    X_sample = X_pool[selected_indices]\n",
    "    y_sample = y_pool[selected_indices]\n",
    "\n",
    "    # Update the training set with new samples\n",
    "    X_train = np.vstack([X_train, X_sample])\n",
    "    y_train = np.concatenate([y_train, y_sample])\n",
    "\n",
    "    # Convert updated training data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    # Remove the selected samples from the pool\n",
    "    X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "    # Stop criteria based on the pool size or maximum training size\n",
    "    if len(X_pool) == 0 or len(X_pool) < sample_size or len(X_train) >= 60000:\n",
    "        break\n",
    "\n",
    "# Print the final classification report\n",
    "y_pred_final = evaluate_nn(model, X_test_tensor)\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysisvisualizationfiles-qa5NmZKI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
