{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score,classification_report,pairwise_distances_argmin_min,silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from preprocess_data import preprocess_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances,pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = preprocess_data()\n",
    "\n",
    "data['num_genres'] = data['genre'].apply(len)\n",
    "data = data[data['num_genres'] == 1]\n",
    "\n",
    "descriptions = data['description_processed'].tolist()\n",
    "genres = data['genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['genre_name'] = data['genre'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description_processed</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_genres</th>\n",
       "      <th>genre_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>two men high rank wooing beautiful famous eque...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>richard gloucester uses manipulation murder ga...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dr friedrich wife becomes mentally unstable re...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>single mother separated children due financial...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>john howard payne miserable point life writes ...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81255</th>\n",
       "      <td>81255</td>\n",
       "      <td>ali illness changes life also father father be...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81259</th>\n",
       "      <td>81259</td>\n",
       "      <td>deadly accident paolo comes back earth minutes...</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81262</th>\n",
       "      <td>81262</td>\n",
       "      <td>five women history different generations fears...</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81269</th>\n",
       "      <td>81269</td>\n",
       "      <td>set trivandrum story ottam unfolds day progres...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81270</th>\n",
       "      <td>81270</td>\n",
       "      <td>unusual bond sixty year old dalit worker azhag...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                              description_processed     genre  \\\n",
       "1          1  two men high rank wooing beautiful famous eque...   [Drama]   \n",
       "8          8  richard gloucester uses manipulation murder ga...   [Drama]   \n",
       "9          9  dr friedrich wife becomes mentally unstable re...   [Drama]   \n",
       "11        11  single mother separated children due financial...   [Drama]   \n",
       "15        15  john howard payne miserable point life writes ...   [Drama]   \n",
       "...      ...                                                ...       ...   \n",
       "81255  81255  ali illness changes life also father father be...   [Drama]   \n",
       "81259  81259  deadly accident paolo comes back earth minutes...  [Comedy]   \n",
       "81262  81262  five women history different generations fears...  [Comedy]   \n",
       "81269  81269  set trivandrum story ottam unfolds day progres...   [Drama]   \n",
       "81270  81270  unusual bond sixty year old dalit worker azhag...   [Drama]   \n",
       "\n",
       "       num_genres genre_name  \n",
       "1               1      Drama  \n",
       "8               1      Drama  \n",
       "9               1      Drama  \n",
       "11              1      Drama  \n",
       "15              1      Drama  \n",
       "...           ...        ...  \n",
       "81255           1      Drama  \n",
       "81259           1     Comedy  \n",
       "81262           1     Comedy  \n",
       "81269           1      Drama  \n",
       "81270           1      Drama  \n",
       "\n",
       "[17999 rows x 5 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(data['genre_name'] == 'Drama') | (data['genre_name'] == 'Comedy')]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com = data[data['genre_name'] == 'Comedy'].head(6000)\n",
    "#df_hor = data[data['genre_name'] == 'Horror'].head(2000)\n",
    "data = data[data['genre_name'] == 'Drama'].head(6000)\n",
    "data = pd.concat([data, df_com])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre_name\n",
       "Drama     6000\n",
       "Comedy    6000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['genre_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = data['description_processed'].tolist()\n",
    "genres = data['genre'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Encode the genres as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(genres)\n",
    "\n",
    "# Load a pre-trained sentence-transformer model to convert text to embeddings\n",
    "#model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 762/762 [01:04<00:00, 11.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert descriptions to vector embeddings\n",
    "X = model.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the embeddings for cosine similarity\n",
    "X_train_full_normalized = normalize(X_train_full, axis=1, norm='l2')\n",
    "X_test_normalized = normalize(X_test, axis=1, norm='l2')\n",
    "\n",
    "# Active Learning parameters\n",
    "initial_train_size = 100\n",
    "iterations = 20\n",
    "sample_size = 250\n",
    "uncertainty_threshold = 0.2\n",
    "budget_per_iteration = 250    \n",
    "\n",
    "#Selecting initial training set randomly\n",
    "np.random.seed(42)\n",
    "pool_indices = np.random.choice(len(X_train_full_normalized), initial_train_size, replace=False)\n",
    "X_train = X_train_full_normalized[pool_indices]\n",
    "y_train = np.array(y_train_full)[pool_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "pool_indices = np.random.choice(len(X_train_full_normalized), initial_train_size, replace=False)\n",
    "X_train = X_train_full_normalized[pool_indices]\n",
    "y_train = np.array(y_train_full)[pool_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-28 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-28 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-28 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-28 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-28 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-28 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-28 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-28 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-28 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-28 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-28 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-28 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-28 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-28 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-28 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', random_state=42)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the FAISS Index for Cosine Similarity\n",
    "faiss.omp_set_num_threads(12)\n",
    "embedding_dim = X_train_full_normalized.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # Inner product index for cosine similarity\n",
    "index.add(X_train_full_normalized)  # Add all normalized vectors to the index\n",
    "\n",
    "# Remaining pool of indices\n",
    "remaining_indices = list(set(range(len(X_train_full_normalized))) - set(pool_indices))\n",
    "\n",
    "# Initialize the classifier\n",
    "#clf = RandomForestClassifier(random_state=42)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=42)\n",
    "clf.partial_fit(X_train, y_train, classes=np.unique(y_train_full))\n",
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool, X_train, y_pool, y_train = train_test_split(X_train_full, y_train_full, test_size=0.00255, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Accuracy = 0.3765\n",
      "Iteration 2: Accuracy = 0.4044\n",
      "Iteration 3: Accuracy = 0.4874\n",
      "Iteration 4: Accuracy = 0.5798\n",
      "Iteration 5: Accuracy = 0.5769\n",
      "Iteration 6: Accuracy = 0.5820\n",
      "Iteration 7: Accuracy = 0.6081\n",
      "Iteration 8: Accuracy = 0.6122\n",
      "Iteration 9: Accuracy = 0.6225\n",
      "Iteration 10: Accuracy = 0.6138\n",
      "Iteration 11: Accuracy = 0.6218\n",
      "Iteration 12: Accuracy = 0.6007\n",
      "Iteration 13: Accuracy = 0.6282\n",
      "Iteration 14: Accuracy = 0.6323\n",
      "Iteration 15: Accuracy = 0.6286\n",
      "Iteration 16: Accuracy = 0.6274\n",
      "Iteration 17: Accuracy = 0.6366\n",
      "Iteration 18: Accuracy = 0.6354\n",
      "Iteration 19: Accuracy = 0.6327\n",
      "Iteration 20: Accuracy = 0.6245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.39      0.41       134\n",
      "           1       0.40      0.09      0.14        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.53      0.74      0.62      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.72      0.70      0.71      2233\n",
      "           7       1.00      0.02      0.04        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.63      0.69      0.66       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.86      0.33      0.47        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.27      0.08      0.13       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.76      0.67      0.71       128\n",
      "\n",
      "    accuracy                           0.62      4871\n",
      "   macro avg       0.28      0.19      0.19      4871\n",
      "weighted avg       0.60      0.62      0.60      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Incrementally train the classifier with the current training set\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate performance on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}: Accuracy = {accuracy_scores[-1]:.4f}\")\n",
    "\n",
    "    # Random sampling of new data points\n",
    "    sample_size = min(sample_size, len(X_pool))\n",
    "    selected_indices = np.random.choice(len(X_pool), size=sample_size, replace=False)\n",
    "    X_sample = X_pool[selected_indices]\n",
    "\n",
    "    # Handle potential differences in data type or format\n",
    "    try:\n",
    "        y_sample = y_pool[selected_indices]\n",
    "    except:\n",
    "        y_sample = y_pool.iloc[selected_indices]\n",
    "    \n",
    "    # Update the training set with new samples\n",
    "    X_train = np.vstack([X_train, X_sample])\n",
    "    y_train = np.concatenate([y_train, y_sample])\n",
    "\n",
    "    # Remove the selected samples from the pool\n",
    "    X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "    # Stop criteria based on the pool size or maximum training size\n",
    "    if len(X_pool) == 0 or len(X_pool) < sample_size or len(X_train) >= 60000:\n",
    "        break\n",
    "\n",
    "# Print the final classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN As Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.9619\n",
      "Iteration 2: Test Accuracy = 0.9285\n",
      "Iteration 3: Test Accuracy = 0.9632\n",
      "Iteration 4: Test Accuracy = 0.9619\n",
      "Iteration 5: Test Accuracy = 0.9585\n",
      "Iteration 6: Test Accuracy = 0.9636\n",
      "Iteration 7: Test Accuracy = 0.9615\n",
      "Iteration 8: Test Accuracy = 0.9628\n",
      "Iteration 9: Test Accuracy = 0.9632\n",
      "Iteration 10: Test Accuracy = 0.9623\n",
      "Iteration 11: Test Accuracy = 0.9623\n",
      "Iteration 12: Test Accuracy = 0.9628\n",
      "Iteration 13: Test Accuracy = 0.9628\n",
      "Iteration 14: Test Accuracy = 0.9623\n",
      "Iteration 15: Test Accuracy = 0.9623\n",
      "Iteration 16: Test Accuracy = 0.9632\n",
      "Iteration 17: Test Accuracy = 0.9628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 451 points to 20 centroids: please provide at least 780 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18: Test Accuracy = 0.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 147 points to 20 centroids: please provide at least 780 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19: Test Accuracy = 0.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 39 points to 20 centroids: please provide at least 780 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: Test Accuracy = 0.9615\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Define number of clusters\n",
    "    num_clusters = 20\n",
    "    \n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 100  # Number of iterations for clustering\n",
    "    \n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatIP(embedding_dim)  # Initialize FAISS index for clustering\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "    # Select samples from each cluster using a hybrid strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            num_to_select_from_cluster = min(100, int(len(cluster_indices)/2))\n",
    "            # Find the closest sample to the cluster center\n",
    "            cluster_center = centroids[cluster].reshape(1, -1)\n",
    "            distances, _ = index_flat.search(cluster_center, len(cluster_indices))\n",
    "            closest_samples = [cluster_indices[i] for i in distances[0].argsort()[:num_to_select_from_cluster]]\n",
    "            selected_indices.extend(closest_samples)\n",
    "\n",
    "            # Diversity Sampling: Select most diverse samples in the cluster\n",
    "            cluster_data = X_train_full_normalized[cluster_indices]\n",
    "            pairwise_distances_matrix = pairwise_distances(cluster_data)\n",
    "            diversity_scores = pairwise_distances_matrix.mean(axis=1)\n",
    "            most_diverse_indices = np.argsort(-diversity_scores)[:num_to_select_from_cluster]\n",
    "            diverse_samples = [cluster_indices[i] for i in most_diverse_indices]\n",
    "            selected_indices.extend(diverse_samples)\n",
    "\n",
    "    '''# Uncertainty sampling\n",
    "    if len(remaining_indices) > 0:\n",
    "        # Predict probabilities for remaining samples\n",
    "        probs = clf.predict_proba(X_train_full_normalized[remaining_indices])\n",
    "        # Calculate uncertainty as 1 - max probability\n",
    "        uncertainty = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Determine the number of uncertain samples to select\n",
    "        num_uncertain_samples = int(sample_size * uncertainty_threshold)\n",
    "        if num_uncertain_samples > len(remaining_indices):\n",
    "            num_uncertain_samples = len(remaining_indices)\n",
    "        \n",
    "        # Select the most uncertain samples\n",
    "        uncertain_indices = np.argsort(-uncertainty)[:num_uncertain_samples]\n",
    "        \n",
    "        # Add uncertain samples to selected indices\n",
    "        selected_indices.extend([remaining_indices[i] for i in uncertain_indices])'''\n",
    "\n",
    "    # Ensure selected indices are unique and limited to the budget per iteration\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "    #print(np.unique(y_train))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.5221\n",
      "Iteration 2: Test Accuracy = 0.5272\n",
      "Iteration 3: Test Accuracy = 0.5847\n",
      "Iteration 4: Test Accuracy = 0.4927\n",
      "Iteration 5: Test Accuracy = 0.5995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5349/3731193401.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Convert remaining indices to the appropriate format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mremaining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_full_normalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mindex_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlatL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize FAISS index for clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Get cluster assignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_assignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, index, weights)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x, index, x_weights)\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0mindex\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0massignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mweight\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNULL\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msize\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \"\"\"\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClustering_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Define number of clusters\n",
    "    num_clusters = min(int(np.sqrt(len(remaining_indices))), len(remaining_indices))\n",
    "    \n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50  # Number of iterations for clustering\n",
    "    \n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)  # Initialize FAISS index for clustering\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "    # Select samples from each cluster using a hybrid strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Find the closest sample to the cluster center\n",
    "            cluster_center = centroids[cluster].reshape(1, -1)\n",
    "            distances, _ = index.search(cluster_center, len(cluster_indices))\n",
    "            closest_sample_index = cluster_indices[distances[0].argmin()]\n",
    "            selected_indices.append(closest_sample_index)\n",
    "\n",
    "            # Diversity Sampling: Select most diverse samples in the cluster\n",
    "            cluster_data = X_train_full_normalized[cluster_indices]\n",
    "            pairwise_distances_matrix = pairwise_distances(cluster_data)\n",
    "            diversity_scores = pairwise_distances_matrix.mean(axis=1)\n",
    "            most_diverse_index = cluster_indices[np.argmax(diversity_scores)]\n",
    "            selected_indices.append(most_diverse_index)\n",
    "\n",
    "    # Uncertainty sampling\n",
    "    if len(remaining_indices) > 0:\n",
    "        # Predict probabilities for remaining samples\n",
    "        probs = clf.predict_proba(X_train_full_normalized[remaining_indices])\n",
    "        # Calculate uncertainty as 1 - max probability\n",
    "        uncertainty = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Determine the number of uncertain samples to select\n",
    "        num_uncertain_samples = int(sample_size * uncertainty_threshold)\n",
    "        if num_uncertain_samples > len(remaining_indices):\n",
    "            num_uncertain_samples = len(remaining_indices)\n",
    "        \n",
    "        # Select the most uncertain samples\n",
    "        uncertain_indices = np.argsort(-uncertainty)[:num_uncertain_samples]\n",
    "        \n",
    "        # Add uncertain samples to selected indices\n",
    "        selected_indices.extend([remaining_indices[i] for i in uncertain_indices])\n",
    "\n",
    "    # Ensure selected indices are unique and limited to the budget per iteration\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "    #print(np.unique(y_train))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.11      0.18        91\n",
      "           1       0.97      1.00      0.98      2272\n",
      "\n",
      "    accuracy                           0.96      2363\n",
      "   macro avg       0.73      0.55      0.58      2363\n",
      "weighted avg       0.95      0.96      0.95      2363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN As Clustering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_focus_ratio = 0.3 \n",
    "dynamic_clusters = 5\n",
    "# Define a threshold to identify minority classes\n",
    "minority_threshold = 100  # You can adjust this threshold based on your dataset\n",
    "\n",
    "# Calculate the original class distribution\n",
    "original_class_distribution = np.bincount(y_train_full)\n",
    "\n",
    "# Identify minority classes based on the threshold\n",
    "minority_classes = [i for i, count in enumerate(original_class_distribution) if count < minority_threshold]\n",
    "# Initialize the dictionary to store class distribution for each cluster\n",
    "cluster_class_distribution = {i: {} for i in range(num_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class Distribution:\n",
      "Class 0: 1602 samples\n",
      "Class 1: 1599 samples\n",
      "Class 2: 1599 samples\n"
     ]
    }
   ],
   "source": [
    "original_class_distribution = np.bincount(y_train_full)\n",
    "print(\"Original Class Distribution:\")\n",
    "for class_label, count in enumerate(original_class_distribution):\n",
    "    print(f\"Class {class_label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.6600\n",
      "Iteration 2: Test Accuracy = 0.5671\n",
      "Iteration 3: Test Accuracy = 0.6717\n",
      "Iteration 4: Test Accuracy = 0.6721\n",
      "Iteration 5: Test Accuracy = 0.6483\n",
      "Iteration 6: Test Accuracy = 0.7000\n",
      "Iteration 7: Test Accuracy = 0.7113\n",
      "Iteration 8: Test Accuracy = 0.7058\n",
      "Iteration 9: Test Accuracy = 0.7229\n",
      "Iteration 10: Test Accuracy = 0.7054\n",
      "Iteration 11: Test Accuracy = 0.7246\n",
      "Iteration 12: Test Accuracy = 0.7350\n",
      "Iteration 13: Test Accuracy = 0.7238\n",
      "Iteration 14: Test Accuracy = 0.7367\n",
      "Iteration 15: Test Accuracy = 0.7150\n",
      "Iteration 16: Test Accuracy = 0.7388\n",
      "Iteration 17: Test Accuracy = 0.7425\n",
      "Iteration 18: Test Accuracy = 0.7421\n",
      "Iteration 19: Test Accuracy = 0.7113\n",
      "Iteration 20: Test Accuracy = 0.7367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71      1189\n",
      "           1       0.71      0.82      0.76      1211\n",
      "\n",
      "    accuracy                           0.74      2400\n",
      "   macro avg       0.74      0.74      0.73      2400\n",
      "weighted avg       0.74      0.74      0.73      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "index_flat = faiss.IndexFlatIP(embedding_dim)\n",
    "for iteration in range(iterations):\n",
    "    # Adjust the number of clusters dynamically\n",
    "    \n",
    "    num_clusters = 2\n",
    "\n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 100\n",
    "\n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat.add(remaining_data)\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "    \n",
    "    # Print class distribution within each cluster\n",
    "    '''print(\"\\nClass distribution within each cluster:\")\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            cluster_labels = [y_train_full[i] for i in cluster_indices]\n",
    "            unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "            cluster_distribution = dict(zip(unique, counts))\n",
    "            print(f\"Cluster {cluster}: {cluster_distribution}\")'''\n",
    "\n",
    "    # Select samples from each cluster using a refined strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "\n",
    "\n",
    "        if cluster_indices:\n",
    "            num_to_select_from_cluster = min(50, int(len(cluster_indices)/3))\n",
    "\n",
    "            cluster_center = centroids[cluster].reshape(1, -1)\n",
    "            distances, _ = index_flat.search(cluster_center, len(cluster_indices))\n",
    "            closest_samples = [cluster_indices[i] for i in distances[0].argsort()[:num_to_select_from_cluster]]\n",
    "            selected_indices.extend(closest_samples)\n",
    "\n",
    "            cluster_data = X_train_full_normalized[cluster_indices]\n",
    "            pairwise_distances_matrix = pairwise_distances(cluster_data)\n",
    "            diversity_scores = pairwise_distances_matrix.mean(axis=1)\n",
    "            most_diverse_indices = np.argsort(-diversity_scores)[:num_to_select_from_cluster]\n",
    "            diverse_samples = [cluster_indices[i] for i in most_diverse_indices]\n",
    "            selected_indices.extend(diverse_samples)\n",
    "\n",
    "            probs = clf.predict_proba(cluster_data)\n",
    "            uncertainty = 1 - np.max(probs, axis=1)\n",
    "\n",
    "            # Prioritize samples with high uncertainty and diversity\n",
    "            sorted_indices = np.argsort(-uncertainty)\n",
    "            ##num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "            selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_to_select_from_cluster]]\n",
    "            selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "    # Ensure unique samples and limit to the budget\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Test Accuracy = 0.6461\n",
      "Iteration 2: Test Accuracy = 0.6440\n",
      "Iteration 3: Test Accuracy = 0.6475\n",
      "Iteration 4: Test Accuracy = 0.6471\n",
      "Iteration 5: Test Accuracy = 0.6500\n",
      "Iteration 6: Test Accuracy = 0.6483\n",
      "Iteration 7: Test Accuracy = 0.6491\n",
      "Iteration 8: Test Accuracy = 0.6489\n",
      "Iteration 9: Test Accuracy = 0.6483\n",
      "Iteration 10: Test Accuracy = 0.6504\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Adjust the number of clusters dynamically\n",
    "    \n",
    "    num_clusters = dynamic_clusters + iteration  # Increase clusters gradually\n",
    "\n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50\n",
    "\n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    D, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    \n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "    \n",
    "    # Print class distribution within each cluster\n",
    "    # print(\"\\nClass distribution within each cluster:\")\n",
    "    # for cluster in range(num_clusters):\n",
    "    #     cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "    #     if cluster_indices:\n",
    "    #         cluster_labels = [y_train_full[i] for i in cluster_indices]\n",
    "    #         unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    #         cluster_distribution = dict(zip(unique, counts))\n",
    "    #         print(f\"Cluster {cluster}: {cluster_distribution}\")\n",
    "\n",
    "    # Select samples from each cluster using a refined strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Select the most uncertain samples within each cluster\n",
    "            cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "            probs = clf.predict_proba(cluster_data)\n",
    "            uncertainty = 1 - np.max(probs, axis=1)\n",
    "\n",
    "            # Calculate diversity by distance from the centroid\n",
    "            distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "            combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "            # Prioritize samples with high uncertainty and diversity\n",
    "            sorted_indices = np.argsort(-combined_scores)\n",
    "            num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "            selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "            selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "    # Ensure unique samples and limit to the budget\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Incrementally train the classifier with new samples\n",
    "    clf.partial_fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier on the test set after each iteration\n",
    "    y_pred = clf.predict(X_test_normalized)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.35      0.42       134\n",
      "           1       0.57      0.09      0.15        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.58      0.69      0.63      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.69      0.78      0.73      2233\n",
      "           7       0.67      0.04      0.07        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.65      0.67      0.66       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.33      0.01      0.02        80\n",
      "          15       0.89      0.29      0.44        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.29      0.09      0.14       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.84      0.65      0.73       128\n",
      "\n",
      "    accuracy                           0.64      4871\n",
      "   macro avg       0.30      0.18      0.20      4871\n",
      "weighted avg       0.61      0.64      0.61      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super (NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train_full_normalized.shape[1]\n",
    "hidden_size = 128  # You can adjust the number of neurons\n",
    "num_classes = len(np.unique(y_train_full))\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "model = NN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, X_train, y_train, optimizer, criterion, batch_size=64, epochs=5):\n",
    "    model.train()\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(model, X_test, batch_size=64):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert data to PyTorch tensor\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_test_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            outputs = model(inputs[0])\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 5 centroids: please provide at least 195 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.9518\n",
      "Epoch [2/5], Loss: 2.9193\n",
      "Epoch [3/5], Loss: 2.8840\n",
      "Epoch [4/5], Loss: 2.8416\n",
      "Epoch [5/5], Loss: 2.8002\n",
      "Iteration 1: Test Accuracy = 0.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 6 centroids: please provide at least 234 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.7380\n",
      "Epoch [2/5], Loss: 2.6956\n",
      "Epoch [3/5], Loss: 2.6078\n",
      "Epoch [4/5], Loss: 2.5587\n",
      "Epoch [5/5], Loss: 2.4624\n",
      "Iteration 2: Test Accuracy = 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3665\n",
      "Epoch [2/5], Loss: 2.2774\n",
      "Epoch [3/5], Loss: 2.2254\n",
      "Epoch [4/5], Loss: 2.1046\n",
      "Epoch [5/5], Loss: 2.0336\n",
      "Iteration 3: Test Accuracy = 0.3837\n",
      "Epoch [1/5], Loss: 1.9457\n",
      "Epoch [2/5], Loss: 1.7915\n",
      "Epoch [3/5], Loss: 1.7427\n",
      "Epoch [4/5], Loss: 1.6912\n",
      "Epoch [5/5], Loss: 1.6481\n",
      "Iteration 4: Test Accuracy = 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.5706\n",
      "Epoch [2/5], Loss: 1.4492\n",
      "Epoch [3/5], Loss: 1.5778\n",
      "Epoch [4/5], Loss: 1.4310\n",
      "Epoch [5/5], Loss: 1.3219\n",
      "Iteration 5: Test Accuracy = 0.4494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3532\n",
      "Epoch [2/5], Loss: 1.3258\n",
      "Epoch [3/5], Loss: 1.2461\n",
      "Epoch [4/5], Loss: 1.2282\n",
      "Epoch [5/5], Loss: 1.1023\n",
      "Iteration 6: Test Accuracy = 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.2506\n",
      "Epoch [2/5], Loss: 1.1766\n",
      "Epoch [3/5], Loss: 1.1076\n",
      "Epoch [4/5], Loss: 1.0224\n",
      "Epoch [5/5], Loss: 0.9852\n",
      "Iteration 7: Test Accuracy = 0.5048\n",
      "Epoch [1/5], Loss: 1.0508\n",
      "Epoch [2/5], Loss: 0.9411\n",
      "Epoch [3/5], Loss: 0.8931\n",
      "Epoch [4/5], Loss: 0.9006\n",
      "Epoch [5/5], Loss: 0.8365\n",
      "Iteration 8: Test Accuracy = 0.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.8622\n",
      "Epoch [2/5], Loss: 0.7847\n",
      "Epoch [3/5], Loss: 0.7749\n",
      "Epoch [4/5], Loss: 0.6680\n",
      "Epoch [5/5], Loss: 0.6938\n",
      "Iteration 9: Test Accuracy = 0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6175\n",
      "Epoch [2/5], Loss: 0.6706\n",
      "Epoch [3/5], Loss: 0.6131\n",
      "Epoch [4/5], Loss: 0.6128\n",
      "Epoch [5/5], Loss: 0.5380\n",
      "Iteration 10: Test Accuracy = 0.5307\n",
      "Epoch [1/5], Loss: 0.5111\n",
      "Epoch [2/5], Loss: 0.5088\n",
      "Epoch [3/5], Loss: 0.5019\n",
      "Epoch [4/5], Loss: 0.4458\n",
      "Epoch [5/5], Loss: 0.4143\n",
      "Iteration 11: Test Accuracy = 0.5332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4124\n",
      "Epoch [2/5], Loss: 0.3874\n",
      "Epoch [3/5], Loss: 0.3813\n",
      "Epoch [4/5], Loss: 0.3685\n",
      "Epoch [5/5], Loss: 0.3462\n",
      "Iteration 12: Test Accuracy = 0.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3186\n",
      "Epoch [2/5], Loss: 0.3059\n",
      "Epoch [3/5], Loss: 0.3153\n",
      "Epoch [4/5], Loss: 0.2884\n",
      "Epoch [5/5], Loss: 0.2595\n",
      "Iteration 13: Test Accuracy = 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2616\n",
      "Epoch [2/5], Loss: 0.2304\n",
      "Epoch [3/5], Loss: 0.2362\n",
      "Epoch [4/5], Loss: 0.2133\n",
      "Epoch [5/5], Loss: 0.2026\n",
      "Iteration 14: Test Accuracy = 0.5317\n",
      "Epoch [1/5], Loss: 0.1929\n",
      "Epoch [2/5], Loss: 0.1895\n",
      "Epoch [3/5], Loss: 0.1675\n",
      "Epoch [4/5], Loss: 0.1728\n",
      "Epoch [5/5], Loss: 0.1577\n",
      "Iteration 15: Test Accuracy = 0.5323\n",
      "Epoch [1/5], Loss: 0.1561\n",
      "Epoch [2/5], Loss: 0.1475\n",
      "Epoch [3/5], Loss: 0.1478\n",
      "Epoch [4/5], Loss: 0.1298\n",
      "Epoch [5/5], Loss: 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: Test Accuracy = 0.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1240\n",
      "Epoch [2/5], Loss: 0.1196\n",
      "Epoch [3/5], Loss: 0.1121\n",
      "Epoch [4/5], Loss: 0.1080\n",
      "Epoch [5/5], Loss: 0.1066\n",
      "Iteration 17: Test Accuracy = 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1051\n",
      "Epoch [2/5], Loss: 0.1019\n",
      "Epoch [3/5], Loss: 0.0967\n",
      "Epoch [4/5], Loss: 0.0986\n",
      "Epoch [5/5], Loss: 0.0923\n",
      "Iteration 18: Test Accuracy = 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0934\n",
      "Epoch [2/5], Loss: 0.0887\n",
      "Epoch [3/5], Loss: 0.0801\n",
      "Epoch [4/5], Loss: 0.0792\n",
      "Epoch [5/5], Loss: 0.0731\n",
      "Iteration 19: Test Accuracy = 0.5332\n",
      "Epoch [1/5], Loss: 0.0720\n",
      "Epoch [2/5], Loss: 0.0711\n",
      "Epoch [3/5], Loss: 0.0700\n",
      "Epoch [4/5], Loss: 0.0684\n",
      "Epoch [5/5], Loss: 0.0666\n",
      "Iteration 20: Test Accuracy = 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "WARNING clustering 14 points to 7 centroids: please provide at least 273 training points\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "for iteration in range(iterations):\n",
    "    # Adjust the number of clusters dynamically\n",
    "    num_clusters = min(dynamic_clusters + iteration, len(remaining_indices) // 2)\n",
    "\n",
    "    # Initialize FAISS clustering\n",
    "    clustering = faiss.Clustering(embedding_dim, num_clusters)\n",
    "    clustering.verbose = False\n",
    "    clustering.niter = 50\n",
    "\n",
    "    # Convert remaining indices to the appropriate format\n",
    "    remaining_data = np.array([X_train_full_normalized[i] for i in remaining_indices]).astype('float32')\n",
    "    index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "    clustering.train(remaining_data, index_flat)\n",
    "\n",
    "    # Get cluster assignments\n",
    "    _, cluster_assignments = index_flat.search(remaining_data, 1)\n",
    "    cluster_assignments = cluster_assignments.flatten()\n",
    "\n",
    "    # Convert FAISS centroids to numpy array\n",
    "    centroids = faiss.vector_to_array(clustering.centroids).reshape(num_clusters, embedding_dim)\n",
    "\n",
    "    # Select samples from each cluster using a refined strategy\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = [i for i, label in zip(remaining_indices, cluster_assignments) if label == cluster]\n",
    "        if cluster_indices:\n",
    "            # Select the most uncertain samples within each cluster\n",
    "            cluster_data = np.array([X_train_full_normalized[i] for i in cluster_indices]).astype('float32')\n",
    "            cluster_data_tensor = torch.tensor(cluster_data, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Get model predictions and uncertainties\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(cluster_data_tensor)\n",
    "                probs = nn.functional.softmax(outputs, dim=1)\n",
    "                uncertainty = 1 - torch.max(probs, dim=1).values.cpu().numpy()\n",
    "\n",
    "            # Calculate diversity by distance from the centroid\n",
    "            distances_from_centroid = np.linalg.norm(cluster_data - centroids[cluster], axis=1)\n",
    "            combined_scores = uncertainty + distances_from_centroid\n",
    "\n",
    "            # Prioritize samples with high uncertainty and diversity\n",
    "            sorted_indices = np.argsort(-combined_scores)\n",
    "            num_samples = min(int(len(cluster_indices) * uncertainty_threshold), len(cluster_indices))\n",
    "            selected_cluster_indices = [cluster_indices[i] for i in sorted_indices[:num_samples]]\n",
    "            selected_indices.extend(selected_cluster_indices)\n",
    "\n",
    "    # Ensure unique samples and limit to the budget\n",
    "    selected_indices = list(set(selected_indices))\n",
    "    if len(selected_indices) > budget_per_iteration:\n",
    "        selected_indices = selected_indices[:budget_per_iteration]\n",
    "\n",
    "    # Add selected samples to the training set\n",
    "    X_train = np.vstack((X_train, X_train_full_normalized[selected_indices]))\n",
    "    y_train = np.concatenate((y_train, np.array(y_train_full)[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from the pool\n",
    "    remaining_indices = list(set(remaining_indices) - set(selected_indices))\n",
    "\n",
    "    # Update FAISS Index with new training data\n",
    "    index_flat.add(X_train_full_normalized[selected_indices])\n",
    "\n",
    "    # Convert updated training data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    # Incrementally train the neural network with the new samples\n",
    "    train_nn(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size=32, epochs=5)\n",
    "\n",
    "    # Evaluate the neural network on the test set after each iteration\n",
    "    X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "    y_pred = evaluate_nn(model, X_test_tensor)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Test Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy after all iterations: 0.5325\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = evaluate_nn(model, X_test_normalized)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "print(f\"Final Test Accuracy after all iterations: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.9887\n",
      "Epoch [2/5], Loss: 2.9415\n",
      "Epoch [3/5], Loss: 2.9004\n",
      "Epoch [4/5], Loss: 2.8694\n",
      "Epoch [5/5], Loss: 2.8125\n",
      "Iteration 1: Accuracy = 0.5182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       134\n",
      "           1       0.00      0.00      0.00        47\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.49      0.52      0.50      1371\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.53      0.81      0.64      2233\n",
      "           7       0.00      0.00      0.00        53\n",
      "           8       0.00      0.00      0.00        15\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00       406\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        16\n",
      "          13       0.00      0.00      0.00        23\n",
      "          14       0.00      0.00      0.00        80\n",
      "          15       0.00      0.00      0.00        55\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00       225\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00       128\n",
      "\n",
      "    accuracy                           0.52      4871\n",
      "   macro avg       0.05      0.07      0.06      4871\n",
      "weighted avg       0.38      0.52      0.44      4871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_5349/1057497763.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/student/.cache/pypoetry/virtualenvs/dataanalysisvisualizationfiles-qa5NmZKI-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Active Learning Loop\n",
    "accuracy_scores = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Incrementally train the neural network with the current training set\n",
    "    train_nn(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size=32, epochs=5)\n",
    "    \n",
    "    # Predict and evaluate performance on the test set\n",
    "    y_pred = evaluate_nn(model, X_test_tensor)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Iteration {iteration + 1}: Accuracy = {accuracy_scores[-1]:.4f}\")\n",
    "\n",
    "    # Predict probabilities on the pool data to measure uncertainty\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_pool_tensor = torch.tensor(X_pool, dtype=torch.float32).to(device)\n",
    "        pool_probs = nn.functional.softmax(model(X_pool_tensor), dim=1)\n",
    "        uncertainty = 1 - torch.max(pool_probs, dim=1).values.cpu().numpy()\n",
    "\n",
    "    # Select samples based on uncertainty\n",
    "    sample_size = min(sample_size, len(X_pool))\n",
    "    selected_indices = np.argsort(-uncertainty)[:sample_size]\n",
    "    X_sample = X_pool[selected_indices]\n",
    "    y_sample = y_pool[selected_indices]\n",
    "\n",
    "    # Update the training set with new samples\n",
    "    X_train = np.vstack([X_train, X_sample])\n",
    "    y_train = np.concatenate([y_train, y_sample])\n",
    "\n",
    "    # Convert updated training data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "    # Remove the selected samples from the pool\n",
    "    X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "    # Stop criteria based on the pool size or maximum training size\n",
    "    if len(X_pool) == 0 or len(X_pool) < sample_size or len(X_train) >= 60000:\n",
    "        break\n",
    "\n",
    "# Print the final classification report\n",
    "y_pred_final = evaluate_nn(model, X_test_tensor)\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataanalysisvisualizationfiles-qa5NmZKI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
